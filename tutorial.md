# üìò Gu√≠a Maestra: Construyendo un Data Lakehouse con GCP & Python

¬°Bienvenido/a! Est√°s a punto de construir una plataforma de datos profesional. Esta gu√≠a no es solo un recetario de comandos; es un recorrido dise√±ado para que **entiendas** cada decisi√≥n arquitect√≥nica.

Utilizaremos un stack moderno y demandado en la industria: **Google Cloud Platform (GCP)**, **Terraform** (Infraestructura como C√≥digo) y **Python** con **Polars**.

---

## üéØ Objetivo del Proyecto

Simularemos un entorno de producci√≥n real para una empresa de taxis (NYC Taxi).
**Tu misi√≥n:** Crear un sistema automatizado que ingeste, procese y almacene datos masivos de viajes, permitiendo an√°lisis r√°pidos y eficientes.

**Lo que aprender√°s:**
*   üèóÔ∏è **IaC**: C√≥mo levantar infraestructura sin hacer clics manuales.
*   üõ°Ô∏è **Seguridad**: Manejo de credenciales y roles.
*   üêç **Python Moderno**: Uso de tipos est√°ticos, POO y librer√≠as de alto rendimiento (Polars).
*   ‚òÅÔ∏è **Cloud Engineering**: Conceptos de Data Lake vs Data Warehouse.

---

## üìã Prerrequisitos

Antes de empezar, necesitamos preparar tu "caja de herramientas". Aseg√∫rate de tener esto instalado en tu entorno (WSL2, Linux o macOS):

*   **Git**: Para guardar tu progreso y versionar el c√≥digo.
*   **Google Cloud CLI (`gcloud`)**: El "control remoto" de GCP desde tu terminal.
*   **Terraform**: El alba√±il que construir√° tu infraestructura.
*   **Python 3.9+**: El cerebro de nuestra l√≥gica.
*   **uv**: Un gestor de paquetes ultra-r√°pido (lo instalaremos juntos si no lo tienes).

---

## üèóÔ∏è Fase 1: Cimientos y Entorno Local

Al igual que un edificio necesita cimientos s√≥lidos, un proyecto de software necesita una estructura ordenada y segura.

### 1. El "Scaffolding" (Andamiaje)

Vamos a crear una estructura de carpetas profesional. Separaremos la **infraestructura** (el "hardware" virtual) del **c√≥digo** (la l√≥gica de negocio).

Ejecuta esto en tu terminal:

```bash
# 1. Crear la carpeta ra√≠z del proyecto
mkdir nyc-taxi-lakehouse
cd nyc-taxi-lakehouse

# 2. Inicializar Git (Tu bit√°cora de cambios)
git init
git branch -M main

# 3. Crear la estructura de directorios
mkdir -p infrastructure/terraform   # Aqu√≠ vivir√° la definici√≥n de la nube
mkdir -p gcp_credentials            # üîí Aqu√≠ guardaremos las llaves (¬°SECRETO!)
mkdir -p src/ingestion              # C√≥digo Python para descargar datos
mkdir -p dags                       # Orquestaci√≥n (Airflow)
mkdir -p dbt_project                # Transformaci√≥n de datos (SQL)
mkdir -p .github/workflows          # Automatizaci√≥n CI/CD
mkdir -p tests                      # Pruebas de calidad
```

### 2. Seguridad Primero (`.gitignore`)

> [!IMPORTANT]
> **Regla de Oro**: Las credenciales (claves, contrase√±as) **NUNCA** se suben a Git.

Creamos un archivo `.gitignore` para decirle a Git qu√© archivos ignorar:

```text
# --- Python (Archivos temporales y entornos) ---
.venv/
venv/
__pycache__/
*.pyc

# --- Configuraci√≥n Local ---
.env        # Variables de entorno (claves)
.DS_Store   # Basura de macOS

# --- Google Cloud (¬°CR√çTICO!) ---
gcp_credentials/
*.json

# --- Terraform (Estado de la infraestructura) ---
.terraform/
.terraform.lock.hcl
*.tfstate
*.tfstate.backup
```

### 3. Gesti√≥n de Dependencias con `uv`

Usaremos `uv` en lugar de `pip`. Imagina que `pip` es un instalador manual y `uv` es un equipo de instalaci√≥n automatizado de alta velocidad.

```bash
# 1. Inicializar el gestor de paquetes
uv init

# 2. Crear un entorno virtual aislado (.venv)
# Esto evita que las librer√≠as de este proyecto choquen con otros.
uv venv

# 3. Activar el entorno
# En Linux/macOS/WSL:
source .venv/bin/activate
# En Windows (PowerShell):
# .venv\Scripts\activate

# 4. Instalar librer√≠as iniciales
uv pip install polars pyarrow google-cloud-storage python-dotenv
```

---

## ‚òÅÔ∏è Fase 2: Configuraci√≥n de Google Cloud (El "Bootstrap")

Aunque usaremos Terraform para casi todo, necesitamos un punto de partida manual en la nube (el problema del "huevo y la gallina").

1.  **Crear Proyecto**: Ve a la [Consola de GCP](https://console.cloud.google.com/) y crea un proyecto nuevo (ej. `nyc-lakehouse-prod`).
2.  **Facturaci√≥n**: Aseg√∫rate de que tenga una cuenta de facturaci√≥n activa (GCP te da cr√©dito gratis inicial).
3.  **Habilitar APIs**: Las APIs son los enchufes que permiten controlar los servicios. Busca y habilita:
    *   *Compute Engine API*
    *   *Google Cloud Storage JSON API*
    *   *BigQuery API*
4.  **El Robot Constructor (Service Account)**:
    *   Ve a **IAM y administraci√≥n** > **Cuentas de servicio**.
    *   Crea una cuenta llamada `terraform-runner`.
    *   **Rol**: Dale rol de **Propietario (Owner)** (Para este tutorial simplifica las cosas; en producci√≥n ser√≠amos m√°s estrictos).
    *   **Clave**: Crea una clave **JSON**, desc√°rgala y gu√°rdala en tu carpeta `nyc-taxi-lakehouse/gcp_credentials/` con el nombre `terraform-key.json`.

---

## üõ†Ô∏è Fase 3: Infraestructura como C√≥digo (Terraform)

En lugar de hacer clics en la consola para crear servidores, escribiremos "recetas" de c√≥digo que describen lo que queremos. Terraform leer√° la receta y construir√° todo.

**Ve a la carpeta:** `cd infrastructure/terraform/`

### 1. El Proveedor (`provider.tf`)

Define qui√©n es el proveedor de nube (Google) y c√≥mo autenticarse.

```hcl
terraform {
  required_providers {
    google = {
      source  = "hashicorp/google"
      version = "4.51.0"
    }
  }
}

provider "google" {
  credentials = file("../../gcp_credentials/terraform-key.json")
  project     = var.project_id
  region      = var.region
}
```

### 2. Las Variables (`variables.tf`)

Hacemos el c√≥digo reutilizable. En lugar de escribir "mi-proyecto" en todos lados, usamos variables.

```hcl
variable "project_id" {
  description = "nyc-lakehouse-prod"
  type        = string
}

variable "region" {
  description = "Regi√≥n predeterminada"
  type        = string
  default     = "us-central1" 
}

variable "gcs_bucket_name" {
  description = "Nombre √∫nico del bucket para el Data Lake"
  type        = string
}

variable "bq_dataset_name" {
  description = "Nombre del dataset de BigQuery"
  type        = string
  default     = "nyc_taxi_wh"
}

```

### 3. Los Recursos (`main.tf`)

Aqu√≠ definimos qu√© queremos construir: un **Bucket** (disco duro ilimitado en la nube) y un **Dataset** (base de datos anal√≠tica Big Query).

```hcl
# Data Lake: Google Cloud Storage Bucket
resource "google_storage_bucket" "data_lake" {
  name          = var.gcs_bucket_name
  location      = var.region
  force_destroy = true # Permite borrar el bucket aunque tenga datos (√∫til para dev)

  uniform_bucket_level_access = true
  
  versioning {
    enabled = true
  }

  lifecycle_rule {
    action {
      type = "Delete"
    }
    condition {
      age = 30 # Limpieza autom√°tica de archivos viejos (ahorro de costos)
    }
  }
}

# Data Warehouse: BigQuery Dataset
resource "google_bigquery_dataset" "dataset" {
  dataset_id                 = var.bq_dataset_name
  friendly_name              = "NYC Taxi DWH"
  description                = "Dataset principal para el Lakehouse"
  location                   = var.region
  delete_contents_on_destroy = true # Cuidado en prod, √∫til aqu√≠
}

```

### 4. Tus Valores (`terraform.tfvars`)

Aqu√≠ pones tus datos reales.
**¬°OJO!** Si este archivo tuviera contrase√±as, deber√≠a ir al `.gitignore`.

```hcl
project_id      = "TU_ID_DE_PROYECTO_REAL"       # <--- CAMBIA ESTO
gcs_bucket_name = "nyc-taxi-lakehouse-raw-tunombre" # <--- CAMBIA ESTO (Debe ser √∫nico en todo Google)
region          = "us-central1"
```

### 5. ¬°A Desplegar!

```bash
# 1. Inicializar (descargar plugins)
terraform init

# 2. Planificar (ver qu√© va a pasar)
terraform plan

# 3. Aplicar (construir la infraestructura)
terraform apply
# Escribe 'yes' para confirmar.
```

#### ¬°Felicidades! üéâ Has desplegado tu infraestructura base en Google Cloud.
---

## üíæ Intermedio: Guardando el Progreso (Git)

Git no guarda carpetas vac√≠as. Para mantener nuestra estructura organizada en el repositorio, usaremos un truco: poner un archivo vac√≠o llamado `.gitkeep` en cada carpeta.

```bash
cd ../..  # Volver a la ra√≠z del proyecto

# Crear archivos ancla
touch dags/.gitkeep src/ingestion/.gitkeep dbt_project/.gitkeep tests/.gitkeep

# Guardar todo en Git
git add .
git commit -m "feat: init project structure and infrastructure"

# Subir a GitHub (Configura tu repo remoto primero)
# git remote add origin <TU_URL_DE_GITHUB>
# git push -u origin main
```

---

## üêç Fase 4: El Motor de Ingesti√≥n (Python + POO)

Ahora que tenemos infraestructura, necesitamos datos. Crearemos un programa en Python que:
1.  **Descargue** datos oficiales de los taxis de NYC.
2.  **Valide** y transforme ligeramente los datos.
3.  **Suba** los archivos a nuestro Data Lake en la nube.

### ¬øPor qu√© Python y POO?
Usaremos **Programaci√≥n Orientada a Objetos (Clases)**. Esto hace que el c√≥digo sea modular (piezas de Lego) y f√°cil de probar, a diferencia de un script "espagueti" que hace todo de arriba a abajo.

### Preparaci√≥n de Librer√≠as

Usaremos `uv add` para instalar y registrar las dependencias en `pyproject.toml` (como el `package.json` de Node.js).

```bash
uv add requests polars google-cloud-storage python-dotenv pyarrow
```

### Configuraci√≥n (`.env`)

Crea un archivo `.env` en la ra√≠z. Esto permite cambiar la configuraci√≥n sin tocar el c√≥digo.

```ini
GOOGLE_APPLICATION_CREDENTIALS="gcp_credentials/terraform-key.json"
GCP_PROJECT_ID="tu-project-id-real"
GCS_BUCKET_NAME="nyc-taxi-lakehouse-raw-tunombre" # El mismo que pusiste en Terraform
```

### El C√≥digo (`src/ingestion/ingest_manager.py`)

Crea este archivo. Lee los comentarios en el c√≥digo, explican el "por qu√©" de cada bloque.

```python
import os
import logging
import requests
import polars as pl
from google.cloud import storage
from datetime import datetime
from dotenv import load_dotenv
import sys

# Configuraci√≥n b√°sica de Logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)

class TaxiIngestor:
    """
    Clase responsable de descargar, transformar m√≠nimamente y cargar 
    datos de NYC Taxis al Data Lake (GCS).
    """

    def __init__(self, bucket_name: str):
        """
        Inicializa el cliente de GCS y configura el bucket de destino.
        """
        self.bucket_name = bucket_name  
        self.storage_client = storage.Client()
        self.bucket = self.storage_client.bucket(bucket_name)
        logging.info(f"üîß Ingestor inicializado para bucket: {bucket_name}")

    def download_data(self, year: int, month: int, service_type: str = "yellow") -> str:
        """
        Descarga el archivo Parquet desde la web de NYC TLC a un temporal local.
        Retorna la ruta del archivo local.
        """
        # Formato de URL oficial de NYC TLC: 
        # https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet
        month_str = f"{month:02d}"
        file_name = f"{service_type}_tripdata_{year}-{month_str}.parquet"
        url = f"https://d37ci6vzurychx.cloudfront.net/trip-data/{file_name}"
        local_path = f"/tmp/{file_name}"

        logging.info(f"‚¨áÔ∏è Iniciando descarga desde: {url}")
        
        try:
            response = requests.get(url, stream=True)
            response.raise_for_status() # Lanza error si 404/500

            with open(local_path, "wb") as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            logging.info(f"‚úÖ Archivo descargado en: {local_path}")
            return local_path
            
        except requests.exceptions.RequestException as e:
            logging.error(f"‚ùå Error descargando archivo: {e}")
            raise

    def validate_and_transform(self, file_path: str) -> str:
        """
        Lee el archivo con Polars para validar esquema y a√±ade metadatos de ingesti√≥n.
        Retorna la ruta del archivo procesado listo para subir.
        """
        logging.info("üîÑ Validando y procesando con Polars...")
        
        try:
            # Lazy Loading para eficiencia de memoria
            df = pl.scan_parquet(file_path)
            
            # Agregamos una columna de metadatos: fecha de ingesti√≥n
            # Esto es vital para auditor√≠a en un Data Lake.
            df_processed = df.with_columns(
                pl.lit(datetime.now()).alias("ingestion_timestamp")
            )

            # Materializamos (collect) y guardamos de nuevo optimizado
            output_path = file_path.replace(".parquet", "_processed.parquet")
            df_processed.collect().write_parquet(output_path)
            
            logging.info(f"‚ú® Transformaci√≥n completada. Filas procesadas.")
            return output_path

        except Exception as e:
            logging.error(f"‚ùå Error procesando con Polars: {e}")
            raise

    def upload_to_gcs(self, local_path: str, destination_blob_name: str):
        """
        Sube el archivo procesado al Data Lake (GCS).
        """
        logging.info(f"‚òÅÔ∏è Subiendo {local_path} a gs://{self.bucket_name}/{destination_blob_name}")
        
        try:
            blob = self.bucket.blob(destination_blob_name)
            blob.upload_from_filename(local_path)
            logging.info("üöÄ Carga a GCS exitosa.")
        except Exception as e:
            logging.error(f"‚ùå Error subiendo a GCS: {e}")
            raise

    def clean_local(self, *files):
        """Borra archivos temporales para mantener el contenedor/entorno limpio."""
        for f in files:
            if os.path.exists(f):
                os.remove(f)
        logging.info("üßπ Limpieza de archivos temporales completada.")

if __name__ == "__main__":
    # Cargar variables de entorno
    load_dotenv()
    
    BUCKET = os.getenv("GCS_BUCKET_NAME")
    if not BUCKET:
        raise ValueError("La variable GCS_BUCKET_NAME no est√° definida en .env")

    # Ejecuci√≥n de prueba
    ingestor = TaxiIngestor(bucket_name=BUCKET)
    
    # Probamos con Enero 2024 (Yellow Taxis)
    YEAR = 2024
    MONTH = 1
    
    try:
        raw_file = ingestor.download_data(YEAR, MONTH)
        processed_file = ingestor.validate_and_transform(raw_file)
        
        # Estructura de carpeta tipo Hive: year=YYYY/month=MM/file.parquet
        gcs_path = f"raw/yellow_tripdata/{YEAR}/{MONTH:02d}/data.parquet"
        
        ingestor.upload_to_gcs(processed_file, gcs_path)
        
        # Limpieza
        ingestor.clean_local(raw_file, processed_file)
        
    except Exception as main_error:
        logging.critical(f"üíÄ El proceso fall√≥: {main_error}")
        exit(1)

```

### üß† Conceptos Clave de este C√≥digo

1.  **Atomicidad**: Dividimos el problema en funciones peque√±as (`download`, `transform`, `upload`). Si algo falla, sabemos exactamente d√≥nde.
2.  **Metadatos**: Agregamos `ingestion_timestamp`. En el futuro, si encuentras un error en los datos, podr√°s saber exactamente cu√°ndo entraron al sistema.
3.  **Particionamiento**: No tiramos los archivos en una pila gigante. Los organizamos en carpetas `a√±o/mes/`. Esto har√° que las consultas en BigQuery sean **mucho m√°s baratas y r√°pidas**.

### üß™ Prueba de Fuego

Vamos a ejecutarlo. Aseg√∫rate de estar en tu entorno virtual (`source .venv/bin/activate`).

```bash
python src/ingestion/ingest_manager.py
```

Si todo sale bien, ver√°s los logs verdes y, si vas a tu consola de Google Cloud Storage, ¬°ver√°s tu archivo Parquet esper√°ndote en la nube!

### üìù Guardar Cambios (Git)

No olvides guardar tu trabajo duro.

```bash
git add src/ingestion/ingest_manager.py pyproject.toml uv.lock .env.example
# Nota: No agregues el .env real, crea un .env.example sin claves si quieres compartir la estructura.

git commit -m "feat: implement robust ingestion script with polars and gcs upload"
git push
```

¬°Excelente trabajo! Has construido la primera tuber√≠a de datos de tu Lakehouse. üöÄ